<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Container on Mark&#39;s Blog</title>
    <link>http://mlavi.github.io/tags/container/</link>
    <description>Recent content in Container on Mark&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Sat, 04 Apr 2015 07:55:39 -0700</lastBuildDate>
    <atom:link href="http://mlavi.github.io/tags/container/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Container Infrastructure Strategy</title>
      <link>http://mlavi.github.io/post/container-infrastructure-strategy/</link>
      <pubDate>Sat, 04 Apr 2015 07:55:39 -0700</pubDate>
      
      <guid>http://mlavi.github.io/post/container-infrastructure-strategy/</guid>
      <description>

&lt;p&gt;In these early years of containers, &amp;ldquo;heavy containers&amp;rdquo; represent a typical approach which
resembles virtual machines, includes the operating system user land, and desires configuration
management. Does this represent the opposite of container promise and immutable infrastructure?&lt;/p&gt;

&lt;h3 id=&#34;heavy-containers:9b4c54333bd97e87ce05878dcf4182cb&#34;&gt;Heavy Containers&lt;/h3&gt;

&lt;p&gt;I have been researching containers for years, I encountered &lt;a href=&#34;https://pantheon.io/blog/why-we-built-pantheon-containers-instead-virtual-machines&#34;&gt;an early mention for Drupal CMS hosting&lt;/a&gt;
 probably a year before I heard about Docker. I had worked with chroot jails earlier in my career,
 but &lt;a href=&#34;http://docker.com&#34;&gt;Docker&lt;/a&gt; made LXC containers easy to use, just as &lt;a href=&#34;http://vagrantup.com&#34;&gt;Vagrant&lt;/a&gt;
 had done for Virtual Machines.&lt;/p&gt;

&lt;p&gt;For the sake of simplifying this discussion, I will not discuss dynamic runtime configuration: it is the
subject of a future blog post. We will approximate it via static application configuration.&lt;/p&gt;

&lt;p&gt;An ideal container holds an application and nothing more: the tricky part is defining your application
and its dependencies. If you look at the full stack an application may be composed, you would consider
the data, code, runtime configuration, server facility, and further dependencies. e.g.:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;application:

&lt;ul&gt;
&lt;li&gt;code: /var/www/virtualhost.example.com/micro/service/route&lt;/li&gt;
&lt;li&gt;code libraries and frameworks: /var/www/shared/language/framework-version (potentially implicitly stored with the code base)&lt;/li&gt;
&lt;li&gt;static application configuration data: /var/www/shared/configuration/databasepassword.inc.txt (potentially implicitly stored with the code base)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;runtime facilities: (e.g.: PHP)

&lt;ul&gt;
&lt;li&gt;language runtime binaries: /usr/bin/local/php-5.x&lt;/li&gt;
&lt;li&gt;language runtime configuration: /etc/php.ini&lt;/li&gt;
&lt;li&gt;language runtime dependencies: openssl, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;server facility: (e.g.: Apache-2.4.x)

&lt;ul&gt;
&lt;li&gt;server binaries: /usr/bin/local/apache2**&lt;/li&gt;
&lt;li&gt;server configuration: /etc/apache2/**&lt;/li&gt;
&lt;li&gt;server runtime configuration: /etc/defaults/apache2&lt;/li&gt;
&lt;li&gt;server startup customization: /etc/init/apache2-custom&lt;/li&gt;
&lt;li&gt;server plug-ins: mod_php, mod_ssl, etc.&lt;/li&gt;
&lt;li&gt;server plug-in dependencies: openssl, php, etc.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is desirable to bundle all of these things together as a full stack, static deployment unit,
making your application portable and self-contained (ha ha).
There is a challenge to decide container scope because of the natural tendency
to follow the dependencies and bundle everything into the container, which weighs it down.&lt;/p&gt;

&lt;p&gt;The initial practice many use for their container is a full
Linux operating system distribution, which I call a &amp;ldquo;heavy container,&amp;rdquo; because it contains
everything and resembles an entire virtual machine (VM):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ grep -e FROM -e RUN Dockerfile
FROM ubuntu:14.04.2
RUN apt-get update &amp;amp;&amp;amp; apt-get install -y apache2 php5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This scope (equivalent to a VM) is natural until you are comfortable with containers,
 then you are ready for container re-factoring and layering. There is a notion of linked
 and data containers which I&amp;rsquo;ll explore later, but what would we re-factor and why?&lt;/p&gt;

&lt;h3 id=&#34;container-re-factoring:9b4c54333bd97e87ce05878dcf4182cb&#34;&gt;Container Re-factoring&lt;/h3&gt;

&lt;p&gt;This is an incomplete thought: I will continue and reorganize it.&lt;/p&gt;

&lt;p&gt;You must address your audience. For developers, a development container might have everything
needed for production but also add troubleshooting and development tools. For production, we would use the
lighter version of the developer container by omitting the extras accomplishing a minimal difference
between dev and prod!&lt;/p&gt;

&lt;p&gt;We can accomplish this by separating containers into layers, where each
&lt;a href=&#34;http://docs.docker.com/terms/layer/&#34;&gt;container layer&lt;/a&gt; is a container unto itself and
&lt;a href=&#34;http://docs.docker.com/reference/builder/#from&#34;&gt;they can be added together&lt;/a&gt; to compose a bigger container.&lt;/p&gt;

&lt;p&gt;Therefore, each container layer could represent an area of expertise which models how your organization
manages resources and could progress with different development cadence. e.g.:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;your system administrators might take care of the base OS,&lt;/li&gt;
&lt;li&gt;your network team might address firewall and networking concerns,&lt;/li&gt;
&lt;li&gt;your server developer team might take care of the server runtime and dependencies,&lt;/li&gt;
&lt;li&gt;your DevOps team could address configuration, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reusing the expertise in your organization suggests a good model to start from and
allows independent iteration of each concern by container layer.&lt;/p&gt;

&lt;p&gt;The result is a robust container build that dynamically invokes its dependent layers, allowing each
to iterate as needed, reducing complexity for a single, monolithic build and risk for rapid deployment.&lt;/p&gt;

&lt;p&gt;So let&amp;rsquo;s explore that idea in more detail:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FROM minimalist/OS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because every guest container resides on a container host OS, refactoring should take into account
the container runtime and container host facilities. This becomes interesting if you&amp;rsquo;ve picked
a minimal Linux distribution such as &lt;a href=&#34;http://coreos.com/&#34;&gt;CoreOS&lt;/a&gt;,
&lt;a href=&#34;https://github.com/phusion/passenger-docker#why_use&#34;&gt;Passenger-docker&lt;/a&gt;,
&lt;a href=&#34;http://rancher.com/rancher-os/&#34;&gt;RancherOS&lt;/a&gt;, or etc. because they are optimized to run containers
(i.e. boot to container). This is a clue that we shouldn&amp;rsquo;t need to bind a full OS user land into a container,
although it is awesome that we can and we should use this for developing our first containers
with sshd and more.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FROM server/facility&lt;/li&gt;
&lt;li&gt;FROM server/runtime&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because your server facility or runtime typically do not change unless there is a new release
which contains a security, performance, or feature fix, this is the next candidate for removal
from a container and refactoring to its own container layer. This likely includes the server dependencies (libraries and other run-times).&lt;/p&gt;

&lt;p&gt;However, the run-time configuration of this server facility could be dynamic
so I would consider that data and a layer closer to the application. In fact, it would be
ideal to make dynamic run-time configuration a service and pull it out of the
container entirely, allowing you to reuse this container for all stacks.&lt;/p&gt;

&lt;p&gt;The idea is that the server facility is a separate layer from our application: it iterates
on a different time line and it is an infrastructure engineer concern most likely, so why would one weigh down an application container layer with it?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FROM operations/facilities&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Facilities such as distributed or centralized logging, metrics, and monitoring are external to the application.
They may be comprised of libraries, frameworks, clients, and their configuration.  These services which could
reside many places, such as: on the container host, a service external to the container host,
 in another container on the same host, or in another container layer.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FROM developer/tools&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Editors, compilers, debuggers, and other diagnostic, utility, and troubleshooting tools and facilities such as SSH,
local logging, metrics, and monitoring are also heavy items and external to the application and should be refactored
into their own container layer.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;FROM application/microserviceX&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, we arrive at the actual code base: this can iterate on its own time line,
triggering a full container rebuild and deployment. Fortunately, each container
layer may be cached, minimizing build time!&lt;/p&gt;

&lt;p&gt;The application has dependencies such as runtime configuration and libraries.
Potentially, those can be externalized from the application to their own container layer which iterates on a separate time line.&lt;/p&gt;

&lt;p&gt;Summary: I would advocate that we compose all of these layers during container build time versus the
standard approach we use the first time we build a container with Docker: using RUN to shell out
and perform installation and configuration tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=M9hBsRUeRdg&#34;&gt;This idea was introduced&lt;/a&gt;
at the &lt;a href=&#34;http://www.meetup.com/coreos/events/215452012/&#34;&gt;CoreOS Meetup&lt;/a&gt;
by James Russell, DevOps engineer at Sony Computer Entertainment America, DevNet Team.
It has been further explored by a talk called
&amp;ldquo;Exploring Strategies for Minimal Containerization&amp;rdquo; by Brian &amp;ldquo;Red beard&amp;rdquo; Harrington,
Principal Architect at CoreOS, but I only found &lt;a href=&#34;https://www.youtube.com/watch?v=P3sO9URqOhE&#34;&gt;a partial reference&lt;/a&gt;
at a recent Container Days keynote.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Most use of RUN represents a duplication of effort that I abhor:&lt;/strong&gt;
&lt;a href=&#34;http://mlavi.github.io/post/build-lamp-stack-with-phing/#migration-of-build-methods:1d44784f593e898273702c2689a01985&#34;&gt;because configuration by shell is something we have done before&lt;/a&gt;
 as a first level approximation to solve the problem. Many of us now use configuration management,
 which is a field comprised of tools known as Ansible, CF Engine, Chef, Puppet, Salt, and more.&lt;/p&gt;

&lt;p&gt;However, configuration management is typically invoked after deployment of a resource during run-time,
but Dockerfiles are invoked in the build domain. I have been exploring this strategic gap between the
VM and the container for months without resolution. I went to ChefConf 2015 and asked this question
but didn&amp;rsquo;t find an answer because I think there is a fundamental divide
between immutable infrastructure (of ephemeral containers) and configuration management of
persistent infrastructure (which is longer lived).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Configuration management of a container host, that makes sense to me.&lt;/li&gt;
&lt;li&gt;Configuration management inside a container guest, that is a heavy container (and doesn&amp;rsquo;t make
ultimate sense to me).&lt;/li&gt;
&lt;li&gt;Configuration management to build a container, that makes sense to me, &lt;em&gt;but I haven&amp;rsquo;t found it
yet.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reusing our configuration management code so we can use it to build a container or a VM makes sense.&lt;/p&gt;

&lt;p&gt;Could it be as simple as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;RUN puppet agent apply&lt;/li&gt;
&lt;li&gt;RUN chef-solo -c ~/solo.rb -j ~/node.json&lt;/li&gt;
&lt;li&gt;etc?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;immutable-infrastructure-versus-configuration-management-build-time-versus-run-time:9b4c54333bd97e87ce05878dcf4182cb&#34;&gt;Immutable Infrastructure versus Configuration Management: build-time versus run-time&lt;/h3&gt;

&lt;p&gt;I want to reuse my configuration management to build a container rather than use shell
commands which represent the lowest level primitive of configuration management functionality.&lt;/p&gt;

&lt;p&gt;One of the benefits of configuration management is that you perform periodic runs
to sync and update configuration, allowing drift correction and auditing of the configured system.
This implies a longer-lived, persistent system and we find that the &amp;ldquo;heavier&amp;rdquo; the system is,
the more useful this model becomes to maintain run state.&lt;/p&gt;

&lt;p&gt;If a system is immutable, constrained to an application code artifact, and continuously deployed
thousands of times a day &amp;ndash; do we need configuration management sync and updates? I like configuration
management to be used during build time for artifacts and only as an option during run-time of those artifacts.
I arrived at this viewpoint recently during a discussion with HashiCorp&amp;rsquo;s CTO, Armon Dadgar, because
it is exactly the use case established with Vagrant to develop and test your configuration management,
but use &lt;a href=&#34;https://packer.io/docs/builders/docker.html&#34;&gt;Packer to exclusively build your container artifact&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It appears this approach is also endorsed by
&lt;a href=&#34;http://www.ansible.com/blog/ansible-and-containers-why-and-how&#34;&gt;Ansible&lt;/a&gt; and
&lt;a href=&#34;https://docs.chef.io/containers.html&#34;&gt;Chef&lt;/a&gt;
while potentially supported by &lt;a href=&#34;https://puppetlabs.com/blog/simplify-managing-docker-puppet&#34;&gt;Puppet&lt;/a&gt;
and &lt;a href=&#34;http://saltstack.com/saltstack-delivers-more-automation-docker-lxc-application-containers/&#34;&gt;SaltStack&lt;/a&gt;,
but I think we can say this needs some more thought and evangelism because I think our entire
community is wrestling with this problem as we transition to understand containers, e.g.:
R.I. Pienaar (author of Marionette Collective (MCo) which is bundled with Puppet)
&lt;a href=&#34;https://www.devco.net/archives/2015/02/24/moving-a-service-from-puppet-to-docker-2.php&#34;&gt;Moving a service from Puppet to Docker&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;container-deployment-and-orchestration:9b4c54333bd97e87ce05878dcf4182cb&#34;&gt;Container Deployment and Orchestration&lt;/h3&gt;

&lt;p&gt;Container deployment and orchestration becomes the next issue: do you use a scheduler like
&lt;a href=&#34;http://mesos.apache.org/&#34;&gt;Apache Mesos&lt;/a&gt;, &lt;a href=&#34;http://kubernetes.io/&#34;&gt;Google Kubernetes&lt;/a&gt;,
or a tool like &lt;a href=&#34;http://terraform.io&#34;&gt;Terraform&lt;/a&gt; to inform your load balancer, DNS, monitoring,
and service discovery and dynamic configuration management systems that a tested official build
artifact is ready to deploy? This is an area where configuration management does not suffice
unless you have some sort of external source driving it.&lt;/p&gt;

&lt;p&gt;Containers in production are easy to deploy when used for the single tier of a web application which
is immutable and hopefully modeled as a micro-service, especially if you reuse the successful model of
HTTP web browsers and servers with capability negotiation and redirects between coexisting different
versions of your API services (if necessary when you do not design forward and backward compatible APIs).&lt;/p&gt;

&lt;p&gt;But the orchestration of a complex roll out of an entire service upgrade representing a medley of
smaller services and applications along with persistent data sources is a higher level problem I
want to tackle soon.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>